{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('qlib': pyenv)",
   "metadata": {
    "interpreter": {
     "hash": "6c9ac720c28485934e83e17ea4f63f5aeb2105365b89324ce61e04d3e2282725"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class SVM(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Support Vector Machine\n",
    "    -----------------------------\n",
    "    This SVM is a subclass of the PyTorch nn module that\n",
    "    implements the Linear  function. The  size  of  each \n",
    "    input sample is 2 and output sample  is 1.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Call the init function of nn.Module\n",
    "        self.fully_connected = nn.Linear(2, 1)  # Implement the Linear function\n",
    "        \n",
    "    def forward(self, x):\n",
    "        fwd = self.fully_connected(x)  # Forward pass\n",
    "        return fwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1  # Learning rate\n",
    "epoch = 10  # Number of epochs\n",
    "batch_size = 1  # Batch size\n",
    "\n",
    "X = torch.FloatTensor(X)  # Convert X and Y to FloatTensors\n",
    "Y = torch.FloatTensor(Y)\n",
    "N = len(Y)  # Number of samples, 500\n",
    "\n",
    "model = SVM()  # Our model\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)  # Our optimizer\n",
    "model.train()  # Our model, SVM is a subclass of the nn.Module, so it inherits the train method\n",
    "for epoch in range(epoch):\n",
    "    perm = torch.randperm(N)  # Generate a set of random numbers of length: sample size\n",
    "    sum_loss = 0  # Loss for each epoch\n",
    "        \n",
    "    for i in range(0, N, batch_size):\n",
    "        x = X[perm[i:i + batch_size]]  # Pick random samples by iterating over random permutation\n",
    "        y = Y[perm[i:i + batch_size]]  # Pick the correlating class\n",
    "        \n",
    "        x = Variable(x)  # Convert features and classes to variables\n",
    "        y = Variable(y)\n",
    "\n",
    "        optimizer.zero_grad()  # Manually zero the gradient buffers of the optimizer\n",
    "        output = model(x)  # Compute the output by doing a forward pass\n",
    "        \n",
    "        loss = torch.mean(torch.clamp(1 - output * y, min=0))  # hinge loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Optimize and adjust weights\n",
    "\n",
    "        sum_loss += loss[0].data.cpu().numpy()  # Add the loss\n",
    "        \n",
    "    print(\"Epoch {}, Loss: {}\".format(epoch, sum_loss[0]))"
   ]
  }
 ]
}